{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40d6e9f4",
   "metadata": {},
   "source": [
    "# Build a content-based file tagging system with LLMs on Amazon Bedrock\n",
    "\n",
    "[Amazon S3](https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html) is a popular object storage service on AWS. You can store any type of file as an object in a S3 bucket. Although you can write files of a specific type or context within a specific directory structure (path) in S3, it will be useful to add metadata to the files like it's content description, owner, context etc. so you can easily retrieve the file that you are looking for. There are two ways to do this:\n",
    "\n",
    "**Option 1: Use the user-defined metadata feature in S3**\n",
    "\n",
    "While uploading an object to a S3 bucket, you can optionally assign user-defined metadata as key-values pairs to the object. This will be stored along with the object. This cannot be added later on to an existing object. The only way to modify object metadata is to make a copy of the object and set the metadata.\n",
    "\n",
    "**Option 2: Store the metadata in an external system with a reference to the object in S3**\n",
    "\n",
    "If you want to set metadata to an existing object in S3 without copying that object or if you want to add to an existing metadata system that already exist, then it will make sense to store the metadata in an external system, like an Amazon DynamoDB table for example. This option is also applicable if the data is stored outside S3 and needs to be tagged with metadata.\n",
    "\n",
    "In both of these options, if you do not know the metadata that describes the data stored in the object, then, you have to read the object, analyze it's content and generate the appropriate metadata. This is where AI can help.\n",
    "\n",
    "[Amazon Bedrock](https://aws.amazon.com/bedrock/) is a fully managed service that offers a choice of high-performing Foundation Models (FMs) from leading AI companies accessible through a single API, along with a broad set of capabilities you need to build generative AI applications, simplifying development while maintaining privacy and security.\n",
    "\n",
    "[Large Language Models (LLMs)](https://en.wikipedia.org/wiki/Large_language_model) are a type of Foundation Model that can take natural langauge as input, with the ability to process and understand it, and produce natural language as the output. LLMs can also can perform tasks like classification, summarization, simplification, entity recognition, etc.\n",
    "\n",
    "This notebook will walk you through the process of analyzing text-content or image-content in files stored in S3 using a Large Language Model (LLM) hosted on [Amazon Bedrock](https://aws.amazon.com/bedrock/), generating metadata based on the content of the files and storing them as key-value pairs (tags) in an [Amazon DynamoDB](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html) table with a reference to the files in S3. In the process, you will learn how to setup the Amazon Bedrock client environment, configure security permissions and use prompt templates. You will also learn how to use Amazon Bedrock's [Converse API](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html) to consistently interact with multiple supported LLMs and easily send documents of various types into the prompts of those LLMs. Finally, you will learn some techniques for analyzing files with mixed content (text and images)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0111293",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b>\n",
    "    <ul>\n",
    "        <li>This notebook should only be run from within an <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html\">Amazon SageMaker Notebook instance</a> or within an <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/studio-updated.html\">Amazon SageMaker Studio Notebook</a>.</li>\n",
    "        <li>At the time of writing this notebook, Amazon Bedrock was only available in <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/bedrock-regions.html\">these supported AWS Regions</a>. If you are running this notebook from any other AWS Region, then you have to change the Amazon Bedrock client's region and/or endpoint URL parameters to one of those supported AWS Regions that has <b>Amazon Nova 1.0 (Lite/Pro) or Anthropic Claude 3 (Haiku/Sonnet/Opus) or Anthropic Claude 3.5 (Sonnet v2/Sonnet) or Meta Llama 3.2 (11B/90B Instruct)</b>. If available, you can use <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/cross-region-inference.html\">cross-region inference</a>. Follow the guidance in the <i>Organize imports</i> section of this notebook.</li>\n",
    "        <li>Amazon Bedrock Converse API supports <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference-supported-models-features.html\">these models</a>. If you intend to use any Converse API supported model other than the ones used in this notebook, make sure both text and image (vision) are supported as input modalities.</li>\n",
    "        <li>At the time of writing this notebook, Converse API supported:\n",
    "            <ul>\n",
    "                <li>Only these document file types -> <b>pdf, csv, doc, docx, xls, xlsx, html, txt, md</b> with up to <b>5 documents</b> in one API call. The max size of a file can be <b>4.5 MB</b>. So make sure your files are one of these types with the right file extension and within the max size limit.</li>\n",
    "                <li>Only these image file types -> <b>png, jpeg, gif, webp</b> with up to <b>20 images</b> in one API call. Each image’s size, height, and width must be no more than <b>3.75 MB, 8000 px, and 8000 px,</b> respectively. So make sure your files are one of these types with the right file extension and within the max size limit.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>This notebook expects files to be readily available in the specified S3 bucket.</li>\n",
    "        <li>This notebook will process <b>either text or image content</b>. If a file contains mixed content (text and image), then, only the text part of the content will be processed; the image part of the content will be ignored.</li>\n",
    "        <li>This notebook is recommended to be run with a minimum instance size of <i>ml.t3.medium</i> and\n",
    "            <ul>\n",
    "                <li>With <i>Amazon Linux 2, Jupyter Lab 4</i> as the platform identifier on an Amazon SageMaker Notebook instance.</li>\n",
    "                <li> (or)\n",
    "                <li>With <i>Data Science 3.0</i> as the image on an Amazon SageMaker Studio Notebook.</li>\n",
    "            <ul>\n",
    "        </li>\n",
    "        <li>At the time of this writing, the most relevant latest version of the Kernel for running this notebook,\n",
    "            <ul>\n",
    "                <li>On an Amazon SageMaker Notebook instance was <i>conda_python3</i></li>\n",
    "                <li>On an Amazon SageMaker Studio Notebook was <i>Python 3</i></li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e88d20",
   "metadata": {},
   "source": [
    "**Table of Contents:**\n",
    "\n",
    "1. [Complete prerequisites](#Complete%20prerequisites)\n",
    "\n",
    "    1. [Check and configure access to the Internet](#Check%20and%20configure%20access%20to%20the%20Internet)\n",
    "\n",
    "    2. [Install required software libraries](#Install%20required%20software%20libraries)\n",
    "    \n",
    "    3. [Configure logging](#Configure%20logging)\n",
    "        \n",
    "        1. [System logs (Optional)](#Configure%20system%20logs%20(Optional))\n",
    "        \n",
    "        2. [Application logs](#Configure%20application%20logs)\n",
    "    \n",
    "    4. [Organize imports](#Organize%20imports)\n",
    "    \n",
    "    5. [Set AWS Region and boto3 config](#Set%20AWS%20Region%20and%20boto3%20config)\n",
    "    \n",
    "    6. [Enable model access in Amazon Bedrock](#Enable%20model%20access%20in%20Amazon%20Bedrock)\n",
    "    \n",
    "    7. [Create common objects](#Create%20common%20objects)\n",
    "    \n",
    "    8. [Check and create the metadata table](#Check%20and%20create%20the%20metadata%20table)\n",
    "    \n",
    "    9. [Check and configure security permissions](#Check%20and%20configure%20security%20permissions)\n",
    "\n",
    " 2. [Download files from S3](#Download%20files%20from%20S3)\n",
    " \n",
    " 3. [Generate and store the file tags in the metadata table](#Generate%20and%20store%20the%20file%20tags%20in%20the%20metadata%20table)\n",
    " \n",
    " 4. [Retrieve the file tags from the metadata table](#Retrieve%20the%20file%20tags%20from%20the%20metadata%20table)\n",
    " \n",
    " 5. [How to analyze mixed content?](#How%20to%20analyze%20mixed%20content)\n",
    " \n",
    " 6. [Cleanup](#Cleanup)\n",
    " \n",
    " 7. [Conclusion](#Conclusion)\n",
    " \n",
    " 8. [Frequently Asked Questions (FAQs)](#FAQs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9fb9d3",
   "metadata": {},
   "source": [
    "##  1. Complete prerequisites <a id ='Complete%20prerequisites'> </a>\n",
    "\n",
    "Check and complete the prerequisites."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e85c39b",
   "metadata": {},
   "source": [
    "###  A. Check and configure access to the Internet <a id ='Check%20and%20configure%20access%20to%20the%20Internet'> </a>\n",
    "This notebook requires outbound access to the Internet to download the required software updates and to download the dataset.  You can either provide direct Internet access (default) or provide Internet access through an [Amazon VPC](https://aws.amazon.com/vpc/).  For more information on this, refer [here](https://docs.aws.amazon.com/sagemaker/latest/dg/appendix-notebook-and-internet-access.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820efd56",
   "metadata": {},
   "source": [
    "### B. Install required software libraries <a id ='Install%20required%20software%20libraries'> </a>\n",
    "This notebook requires the following libraries:\n",
    "* [SageMaker Python SDK version 2.x](https://sagemaker.readthedocs.io/en/stable/v2.html)\n",
    "* [Python 3.10.x](https://www.python.org/downloads/release/python-3100/)\n",
    "* [Boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)\n",
    "\n",
    "Run the following cell to install the required libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb373af",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">  \n",
    "    <b>Note:</b> At the end of the installation, the Kernel will be forcefully restarted immediately. Please wait 10 seconds for the kernel to come back before running the next cell.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7256d4fc-0361-4cee-a548-d9b7e355824f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install boto3==1.35.94\n",
    "!pip install sagemaker==2.237.1\n",
    "\n",
    "import IPython\n",
    "\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b3c44f",
   "metadata": {},
   "source": [
    "### C. Configure logging <a id ='Configure%20logging'> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd5ee37",
   "metadata": {},
   "source": [
    "####  a. System logs (Optional) <a id='Configure%20system%20logs%20(Optional)'></a>\n",
    "\n",
    "System logs refers to the logs generated by the notebook's interactions with the underlying notebook instance. Some examples of these are the logs generated when loading or saving the notebook.\n",
    "\n",
    "These logs are automatically setup when the notebook instance is launched.\n",
    "\n",
    "These logs can be accessed through the [Amazon CloudWatch Logs](https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html) console in the same AWS Region where this notebook is running.\n",
    "* When running this notebook in an Amazon SageMaker Notebook instance, navigate to the following location,\n",
    "    * <i>CloudWatch > Log groups > /aws/sagemaker/NotebookInstances > {notebook-instance-name}/jupyter.log</i>\n",
    "* When running this notebook in an Amazon SageMaker Studio Notebook, navigate to the following locations,\n",
    "    * <i>CloudWatch > Log groups > /aws/sagemaker/studio > {sagmaker-domain-name}/{user-name}/KernelGateway/{notebook-instance-name}</i>\n",
    "    * <i>CloudWatch > Log groups > /aws/sagemaker/studio > {sagmaker-domain-name}/{user-name}/JupyterServer/default</i>\n",
    "\n",
    "If you want to find out the name of the underlying instance where this notebook is running, uncomment the following code cell and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c99c519",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import json\n",
    "\n",
    "notebook_name = ''\n",
    "resource_metadata_path = '/opt/ml/metadata/resource-metadata.json'\n",
    "with open(resource_metadata_path, 'r') as metadata:\n",
    "    notebook_name = (json.load(metadata))['ResourceName']\n",
    "print(\"Notebook instance name: '{}'\".format(notebook_name))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cc0025",
   "metadata": {},
   "source": [
    "####  b. Application logs <a id='Configure%20application%20logs'></a>\n",
    "\n",
    "Application logs refers to the logs generated by running the various code cells in this notebook. To set this up, instantiate the [Python logging service](https://docs.python.org/3/library/logging.html) by running the following cell. You can configure the default log level and format as required.\n",
    "\n",
    "By default, this notebook will only print the logs to the corresponding cell's output console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf96e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "# Set the logging level and format\n",
    "log_level = logging.INFO\n",
    "log_format = '%(asctime)s - %(levelname)s - %(message)s'\n",
    "logging.basicConfig(level=log_level, format=log_format)\n",
    "\n",
    "# Save these in the environment variables for use in the helper scripts\n",
    "os.environ['LOG_LEVEL'] = str(log_level)\n",
    "os.environ['LOG_FORMAT'] = log_format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3bb063",
   "metadata": {},
   "source": [
    "###  D. Organize imports <a id ='Organize%20imports'> </a>\n",
    "\n",
    "Organize all the library and module imports for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764a06b9-812c-4dad-a652-1cb34aa9d8b7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import sys\n",
    "from botocore.config import Config\n",
    "\n",
    "# Import the helper functions from the 'scripts' folder\n",
    "sys.path.append(os.path.join(os.getcwd(), \"scripts\"))\n",
    "#logging.info(\"Updated sys.path: {}\".format(sys.path))\n",
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48ba26b",
   "metadata": {},
   "source": [
    "Print the installed versions of some of the important libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb23f2d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logging.info(\"Python version : {}\".format(sys.version))\n",
    "logging.info(\"Boto3 version : {}\".format(boto3.__version__))\n",
    "logging.info(\"SageMaker Python SDK version : {}\".format(sagemaker.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f63309",
   "metadata": {},
   "source": [
    "###  E. Set AWS Region and boto3 config <a id ='Set%20AWS%20Region%20and%20boto3%20config'> </a>\n",
    "\n",
    "Get the current AWS Region (where this notebook is running) and the SageMaker Session. These will be used to initialize some of the clients to AWS services using the boto3 APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a746eb15",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">  \n",
    "<b>Note:</b> All the AWS services used by this notebook except Amazon Bedrock will use the current AWS Region. For Bedrock, follow the guidance in the next cell.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de7be30",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">  \n",
    "<b>Note:</b> At the time of writing this notebook, Amazon Bedrock was only available in <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/bedrock-regions.html\">these supported AWS Regions</a>. If you are running this notebook from any other AWS Region, then you have to change the Amazon Bedrock client's region and/or endpoint URL parameters to one of those supported AWS Regions that has Amazon Nova 1.0 (Lite/Pro) or Anthropic Claude 3 (Haiku/Sonnet/Opus) or Anthropic Claude 3.5 (Sonnet v2/Sonnet) or Meta Llama 3.2 (11B/90B Instruct). In order to do this, this notebook will use the value specified in the environment variable named <mark>AMAZON_BEDROCK_REGION</mark>. If this is not specified, then the notebook will default to <mark>us-west-2 (Oregon)</mark> for Amazon Bedrock. If available, you can use <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/cross-region-inference.html\">cross-region inference</a>.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a6cb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the AWS Region, SageMaker Session and IAM Role references\n",
    "my_session = boto3.session.Session()\n",
    "logging.info(\"SageMaker Session: {}\".format(my_session))\n",
    "my_iam_role = sagemaker.get_execution_role()\n",
    "logging.info(\"Notebook IAM Role: {}\".format(my_iam_role))\n",
    "my_region = my_session.region_name\n",
    "logging.info(\"Current AWS Region: {}\".format(my_region))\n",
    "\n",
    "# Explicity set the AWS Region for Amazon Bedrock clients\n",
    "AMAZON_BEDROCK_DEFAULT_REGION = \"us-west-2\"\n",
    "br_region = os.environ.get('AMAZON_BEDROCK_REGION')\n",
    "if br_region is None:\n",
    "    br_region = AMAZON_BEDROCK_DEFAULT_REGION\n",
    "elif len(br_region) == 0:\n",
    "    br_region = AMAZON_BEDROCK_DEFAULT_REGION\n",
    "logging.info(\"AWS Region for Amazon Bedrock: {}\".format(br_region))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8484fc",
   "metadata": {},
   "source": [
    "Set the timeout and retry configurations that will be applied to all the boto3 clients used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037155d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the standard time out limits in the boto3 client from 1 minute to 3 minutes\n",
    "# and set the retry limits\n",
    "my_boto3_config = Config(\n",
    "    connect_timeout = (60 * 3),\n",
    "    read_timeout = (60 * 3),\n",
    "    retries = {\n",
    "        'max_attempts': 10,\n",
    "        'mode': 'standard'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984f0e97",
   "metadata": {},
   "source": [
    "###  F. Enable model access in Amazon Bedrock <a id ='Enable%20model%20access%20in%20Amazon%20Bedrock'> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e84ba9d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Note:</b> Before proceeding further with this notebook, you must enable access to the required models on Amazon Bedrock by following the instructions <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html\">here</a>. Otherwise, you will get an authorization error.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed57899f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">  \n",
    "<b>Note:</b> You will have to do this manually after reading the End User License Agreement (EULA) for each of the models that you want to enable. Unless you explicitly disable it, this is a one-time setup for each model in an AWS Region in an AWS account.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19de160b",
   "metadata": {},
   "source": [
    "Run the following cell to print the Amazon Bedrock model access page URL for the AWS Region that was selected earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78213222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Amazon Bedrock model access page URL\n",
    "logging.info(\"Amazon Bedrock model access page - https://{}.console.aws.amazon.com/bedrock/home?region={}#/modelaccess\"\n",
    "             .format(br_region, br_region))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12579ad7",
   "metadata": {},
   "source": [
    "###  G. Create common objects <a id='Create%20common%20objects'></a>\n",
    "\n",
    "To begin with, create the boto3 clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124b256c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Amazon Bedrock client\n",
    "bedrock_client = boto3.client(\"bedrock\", region_name = br_region, endpoint_url = \"https://bedrock.{}.amazonaws.com\"\n",
    "                              .format(br_region), config = my_boto3_config)\n",
    "\n",
    "# Create the Amazon Bedrock runtime client\n",
    "bedrock_rt_client = boto3.client(\"bedrock-runtime\", region_name = br_region, config = my_boto3_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900c771e",
   "metadata": {},
   "source": [
    "In the below cell, modify the value of `model_or_inference profile_id` as needed. For information on retrieving the right model or inference profile id, perform the following.\n",
    "\n",
    "1. Look at the list of [models supported by Converse API](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference-supported-models-features.html). From this, pick the one that support both text and image (vision) as it's input.\n",
    "2. For the model you picked,\n",
    "    - Get the on-demand throughput model id from [here](https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html#model-ids-arns). While running the below cell with that model id, if you get an error that states it is not supported for on-demand throughput, then, proceed to the next step to use the inference profile id.\n",
    "    - Get the inference profile id by following the procedure mentioned [here](https://docs.aws.amazon.com/bedrock/latest/userguide/inference-profiles-view.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693def8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the model id (for Om-Demand) or inference profile id (for Inference Profile)\n",
    "#model_or_inference_profile_id = \"us.amazon.nova-lite-v1:0\"\n",
    "#model_or_inference_profile_id = \"us.amazon.nova-pro-v1:0\"\n",
    "model_or_inference_profile_id = \"anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
    "#model_or_inference_profile_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "#model_or_inference_profile_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "#model_or_inference_profile_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "#model_or_inference_profile_id = \"anthropic.claude-3-opus-20240229-v1:0\"\n",
    "#model_or_inference_profile_id = \"us.meta.llama3-2-11b-instruct-v1:0\"\n",
    "#model_or_inference_profile_id = \"us.meta.llama3-2-90b-instruct-v1:0\"\n",
    "\n",
    "# Specify the Amazon S3 bucket name that will contain all the source files\n",
    "s3_bucket_name = \"<The name of the input S3 bucket>\"\n",
    "\n",
    "# Specify the S3 key prefix to refer to one or more files in the bucket\n",
    "# Note: If the key prefix is empty, then, all the source files in the bucket\n",
    "# will be downloaded with the exact directory structure as in the bucket.\n",
    "s3_key_prefix = \"\"\n",
    "\n",
    "# Specify the name of the DynamoDB table that will contain the metadata\n",
    "ddb_table_name = \"<The name of the DynamoDB table that will store the metadata>\"\n",
    "\n",
    "# Specify the name and location of the prompt templates\n",
    "prompt_templates_dir = os.path.join(os.getcwd(), \"prompt_templates\")\n",
    "system_prompt_template_file = 'system_prompt_template.txt'\n",
    "user_prompt_template_file = 'user_prompt_template.txt'\n",
    "\n",
    "# Specify and create the required output directories\n",
    "data_dir = os.path.join(os.getcwd(), \"data\")\n",
    "os.makedirs(data_dir, exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f230ff",
   "metadata": {},
   "source": [
    "###  H. Check and create the metadata table <a id ='Check%20and%20create%20the%20metadata%20table'> </a>\n",
    "\n",
    "Run the following cell to check and create the specified Amazon DynamoDB table in the current AWS Region. This table will contain the metadata for the objects stored in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a586c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Amazon DynamoDB clients\n",
    "ddb_client = boto3.client(\"dynamodb\", region_name = my_region, config = my_boto3_config)\n",
    "ddb_resource = boto3.resource(\"dynamodb\", region_name = my_region, config = my_boto3_config)\n",
    "\n",
    "# Note: 'check_and_create_metadata_table' is available through ./scripts/helper_functions.py\n",
    "check_and_create_metadata_table(ddb_client, ddb_table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2ee077",
   "metadata": {},
   "source": [
    "###  I. Check and configure security permissions <a id ='Check%20and%20configure%20security%20permissions'> </a>\n",
    "This notebook uses the IAM role attached to the underlying notebook instance.  To view the name of this role, run the following cell. This IAM role should have the following permissions,\n",
    "1. Full access to invoke Large Language Models (LLMs) on Amazon Bedrock.\n",
    "2. Full access to read and write to the Amazon S3 bucket that contains the files to be tagged.\n",
    "3. Full access to read and write to the Amazon DynamoDB table that will contain the metadata.\n",
    "4. Access to write to Amazon CloudWatch Logs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9688f610",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b>  If you are running this notebook as part of a workshop session, by default, all these permissions will be setup.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16baab7",
   "metadata": {},
   "source": [
    "Run the following cell to print the details of the IAM role attached to the underlying notebook instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c64186",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the IAM role ARN and console URL\n",
    "logging.info(\"This notebook's IAM role is '{}'\".format(my_iam_role))\n",
    "arn_parts = my_iam_role.split('/')\n",
    "logging.info(\"Details of this IAM role are available at https://{}.console.aws.amazon.com/iamv2/home?region={}#/roles/details/{}?section=permissions\"\n",
    "             .format(my_region, my_region, arn_parts[len(arn_parts) - 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1a8504",
   "metadata": {},
   "source": [
    "## 2. Download files from S3 <a id ='Download%20files%20from%20S3'> </a>\n",
    "\n",
    "This notebook expects files to be readily available in the specified S3 bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49ed7d1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> At the time of writing this notebook, Converse API supported:\n",
    "    <ul>\n",
    "        <li>Only these document file types -> <b>pdf, csv, doc, docx, xls, xlsx, html, txt, md</b> with up to <b>5 documents</b> in one API call. The max size of a file can be <b>4.5 MB</b>. So make sure your files are one of these types with the right file extension and within the max size limit.</li>\n",
    "        <li>Only these image file types -> <b>png, jpeg, gif, webp</b> with up to <b>20 images</b> in one API call. Each image's size, height, and width must be no more than <b>3.75 MB, 8000 px, and 8000 px,</b> respectively. So make sure your files are one of these types with the right file extension and within the max size limit.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bf93de",
   "metadata": {},
   "source": [
    "Running the following cell will download the files from the S3 bucket to the specified local directory. Existing files will be overwritten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cab240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: 'download_from_s3' is available through ./scripts/helper_functions.py\n",
    "downloaded_file_paths = download_from_s3(data_dir, s3_bucket_name, s3_key_prefix)\n",
    "logging.info(\"Full path(s) of the download file(s): {}\".format(downloaded_file_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fd2c32",
   "metadata": {},
   "source": [
    "## 3. Generate and store the file tags in the metadata table <a id ='Generate%20and%20store%20the%20file%20tags%20in%20the%20metadata%20table'> </a>\n",
    "\n",
    "Loop through all the downloaded files and process as follows:\n",
    "1. Read the file content as bytes.\n",
    "2. Feed the file content bytes along with specific instructions to the specified LLM on Amazon Bedrock through the [Converse API](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html). The instructions should ask the LLM to identify the topic of the content of the file along with the flag that indicates the presence of PII data. Also, specify to the LLM to produce the output as a valid JSON message.\n",
    "3. Insert/Update the response from the LLM in the specified metadata table in DynamoDB using the file's S3 URI as the Partition Key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b868d0c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b>\n",
    "    <ul>\n",
    "        <li>Unsupported file types will be igored.</li>\n",
    "        <li>In document files, only text content will be processed. Image content, if any, will be ignored. If the file contains only image content, then, the document type will be returned as 'empty' or 'blank'.</li>\n",
    "        <li>In image files, the image content will be processed.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075dc35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the list to capture the processed files\n",
    "processed_s3_uris = []\n",
    "\n",
    "# Loop through the downloaded files\n",
    "for downloaded_file_path in downloaded_file_paths:\n",
    "    # Check if the file type is supported by Converse API and process accordingly\n",
    "    # Note: 'is_supported_file_type' is available through ./scripts/helper_functions.py\n",
    "    if is_supported_file_type(downloaded_file_path):\n",
    "        # Prompt the LLM\n",
    "        # Note: 'process_prompt' is available through ./scripts/helper_functions.py\n",
    "        file_metadata = process_prompt(model_or_inference_profile_id, bedrock_rt_client, prompt_templates_dir,\n",
    "                                       system_prompt_template_file, user_prompt_template_file,\n",
    "                                       downloaded_file_path)\n",
    "        # Insert/update the metadata table\n",
    "        file_s3_uri = 's3://' + downloaded_file_path.replace(data_dir, s3_bucket_name)\n",
    "        # Note: 'write_to_metadata_table' is available through ./scripts/helper_functions.py\n",
    "        write_to_metadata_table(ddb_resource, ddb_table_name, file_s3_uri, file_metadata)\n",
    "        # Add to the processed file list\n",
    "        processed_s3_uris.append(file_s3_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0ef817",
   "metadata": {},
   "source": [
    "## 4. Retrieve the file tags from the metadata table <a id='Retrieve%20the%20file%20tags%20from%20the%20metadata%20table'></a>\n",
    "\n",
    "Loop through all the processed files, retrieve their corresponding file tags from the metadata table and print them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7400f5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the processed files\n",
    "for processed_s3_uri in processed_s3_uris:\n",
    "    # Retrieve the metadata\n",
    "    # Note: 'retrieve_from_metadata_table' is available through ./scripts/helper_functions.py\n",
    "    logging.info(\"File: {}, Metadata = {}\".format(processed_s3_uri, retrieve_from_metadata_table(ddb_resource,\n",
    "                                                                                                 ddb_table_name,\n",
    "                                                                                                 processed_s3_uri)))   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e054700",
   "metadata": {},
   "source": [
    "## 5. How to analyze mixed content? <a id='How%20to%20analyze%20mixed%20content'></a>\n",
    "\n",
    "Here are some techniques for analyzing mixed content (text and image) files. In all these, you will have to use the relevant third-party libraries to read the files of specific types like pdf, doc, docx, xls, xlsx etc.\n",
    "\n",
    "**Technique 1**: Prior to invoking the LLM using the Amazon Bedrock Converse API, convert every page in the document to an image. This will be like taking a screenshot of every page in the document. When you do this, you will be dealing with only images and no text at all. Now, you can invoke the LLM using the Converse API by specifying the content type as 'image' and pass in the image bytes. Make sure you pick a LLM that supports [Vision with Converse API](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference-supported-models-features.html). You can refer to the code sample [here](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference-examples.html). At the time of writing this notebook, Converse API supported up to 20 images in one API call. Each image’s size, height, and width must be no more than 3.75 MB, 8000 px, and 8000 px, respectively.\n",
    "\n",
    "**Technique 2**: From each page in a document, read the image content separately as bytes. Then, read the text content (if any) separately. Now, create a list of messages with these info and send them to the LLM in the prompt. For example, if you are using Anthropic Claude 3.x models on Amazon Bedrock that support image (vision) and text in the input, then, you can use the [Messages API](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-messages.html#model-parameters-anthropic-claude-messages-request-response) for this. Note that, depending on the LLM-specific API, you may have to encode the bytes as Base64 and also specify the image type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a1216a",
   "metadata": {},
   "source": [
    "## 6. Cleanup <a id='Cleanup'></a>\n",
    "\n",
    "As a best practice, you should delete AWS resources that are no longer required.  This will help you avoid incurring unncessary costs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c018f3f7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> If you are running this notebook as part of a workshop session, by default, all resources will be cleaned up at the end of the session. If you are running this notebook outside of a workshop session, you can cleanup the resources associated with this notebook by uncommenting the following code cell and running it.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb865606",
   "metadata": {},
   "source": [
    "Running the following cell will delete the following resources:\n",
    "* The DynamoDB table that was created to store the metadata.\n",
    "* The files that were uploaded to the S3 bucket; not the S3 bucket itself.\n",
    "* The files that were downloaded from the S3 bucket to the local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6923a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Note: 'delete_metadata_table' is available through ./scripts/helper_functions.py\n",
    "delete_metadata_table(ddb_client, ddb_table_name)\n",
    "\n",
    "# Create the Amazon S3 client\n",
    "s3_client = boto3.client(\"s3\", region_name = my_region, config = my_boto3_config)\n",
    "# Loop through the downloaded files\n",
    "for downloaded_file_path in downloaded_file_paths:\n",
    "    # Get the S3 key\n",
    "    s3_key = downloaded_file_path.replace(data_dir + os.sep, '')\n",
    "    # Note: 'delete_from_s3' is available through ./scripts/helper_functions.py\n",
    "    delete_from_s3(s3_client, s3_bucket_name, s3_key)\n",
    "    # Note: 'delete_local_file' is available through ./scripts/helper_functions.py\n",
    "    delete_local_file(downloaded_file_path)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bd52a5",
   "metadata": {},
   "source": [
    "## 7. Conclusion <a id='Conclusion'></a>\n",
    "\n",
    "We have now seen how to build a content-based file tagging system using the LLMs on Amazon Bedrock for text-only or image-only content. In the process, we also saw how easy it is to consistently interact with multiple supported LLMs on Amazon Bedrock using the Converse API with the ability to send in documents of various types in the prompt. Finally, we learnt some techniques for analyzing files with mixed content (text and images)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf266cf",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## 8. Frequently Asked Questions (FAQs) <a id='FAQs'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf2c50f",
   "metadata": {},
   "source": [
    "**Q: What AWS services are used in this notebook?**\n",
    "\n",
    "Amazon Bedrock, Amazon DynamoDB, Amazon S3, AWS Identity and Access Management (IAM), Amazon CloudWatch, and Amazon SageMaker Notebook instance (or) Amazon SageMaker Studio Notebook depending on what you use to run the notebook.\n",
    "\n",
    "**Q: Where can I access the FAQs for Amazon Bedrock?**\n",
    "\n",
    "Go [here](https://aws.amazon.com/bedrock/faqs/).\n",
    "\n",
    "**Q: What models are supported by Amazon Bedrock?**\n",
    "\n",
    "Go [here](https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html).\n",
    "\n",
    "**Q: Where can I find customer references for Amazon Bedrock?**\n",
    "\n",
    "Go [here](https://aws.amazon.com/bedrock/testimonials/).\n",
    "\n",
    "**Q: Where can I find resources for prompt engineering?**\n",
    "\n",
    "[Prompt Engineering Guide](https://www.promptingguide.ai/).\n",
    "\n",
    "**Q: Where can I find pricing information for the AWS services used in this notebook?**\n",
    "\n",
    "- Amazon Bedrock pricing - go [here](https://aws.amazon.com/bedrock/pricing/).\n",
    "- Amazon DynamoDB pricing - go [here](https://aws.amazon.com/dynamodb/pricing/).\n",
    "- Amazon S3 pricing - go [here](https://aws.amazon.com/s3/pricing/).\n",
    "- AWS Identity and Access Management (IAM) pricing - free.\n",
    "- Amazon CloudWatch pricing - go [here](https://aws.amazon.com/cloudwatch/pricing/).\n",
    "- Amazon SageMaker Notebook instance (or) Amazon SageMaker Studio Notebook pricing - go [here](https://aws.amazon.com/sagemaker/pricing/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
