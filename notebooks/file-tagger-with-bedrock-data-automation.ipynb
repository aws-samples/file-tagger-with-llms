{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40d6e9f4",
   "metadata": {},
   "source": [
    "# Build a content-based file tagging system using Amazon Bedrock Data Automation\n",
    "\n",
    "[Amazon S3](https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html) is a popular object storage service on AWS. You can store any type of file as an object in a S3 bucket. Although you can write files of a specific type or context within a specific directory structure (path) in S3, it will be useful to add metadata to the files like it's content description, owner, context etc. so you can easily retrieve the file that you are looking for. There are two ways to do this:\n",
    "\n",
    "**Option 1: Use the user-defined metadata feature in S3**\n",
    "\n",
    "While uploading an object to a S3 bucket, you can optionally assign user-defined metadata as key-values pairs to the object. This will be stored along with the object. This cannot be added later on to an existing object. The only way to modify object metadata is to make a copy of the object and set the metadata.\n",
    "\n",
    "**Option 2: Store the metadata in an external system with a reference to the object in S3**\n",
    "\n",
    "If you want to set metadata to an existing object in S3 without copying that object or if you want to add to an existing metadata system that already exist, then it will make sense to store the metadata in an external system, like an Amazon DynamoDB table for example. This option is also applicable if the data is stored outside S3 and needs to be tagged with metadata.\n",
    "\n",
    "In both of these options, if you do not know the metadata that describes the data stored in the object, then, you have to read the object, analyze it's content and generate the appropriate metadata. This is where AI can help.\n",
    "\n",
    "[Amazon Bedrock Data Automation](https://aws.amazon.com/bedrock/bda/) (BDA) is a fully managed service that simplifies the process of extracting valuable insights from unstructured contentâ€”such as documents, images, video, and audio. BDA leverages generative AI to automate the transformation of multi-modal data into structured formats, enabling developers to build applications and automate complex workflows with greater speed and accuracy.\n",
    "\n",
    "This notebook will walk you through the process of analyzing document and image files stored in S3 using [Amazon Bedrock Data Automation](https://aws.amazon.com/bedrock/bda/), generating metadata based on the content of the files and storing them as key-value pairs (tags) in an [Amazon DynamoDB](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html) table with a reference to the files in S3. In the process, you will learn how to setup and interact with Amazon Bedrock Data Automation environment and configure security permissions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0111293",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b>\n",
    "    <ul>\n",
    "        <li>This notebook should only be run from within an <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html\">Amazon SageMaker Notebook instance</a> or within an <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/studio-updated.html\">Amazon SageMaker Studio Notebook</a>.</li>\n",
    "        <li>At the time of writing this notebook, Amazon Bedrock Data Automation (BDA) was available as a preview release in the us-west-2 (Oregon) Region</a>. If you are running this notebook from any other AWS Region, then you have to change the Amazon Bedrock client's region and/or endpoint URL parameters to this Region. Follow the guidance in the <i>Organize imports</i> section of this notebook.</li>\n",
    "        <li>At the time of writing this notebook, BDA supported only <b>PDF, JPEG, PNG, TIFF</b> file extensions for Document/Image Modality. So this notebook will ignore the processing of any other file present in the input S3 bucket.</li>\n",
    "        <li>This notebook expects files to be readily available in the specified input S3 bucket.</li>\n",
    "        <li>This notebook is recommended to be run with a minimum instance size of <i>ml.t3.medium</i> and\n",
    "            <ul>\n",
    "                <li>With <i>Amazon Linux 2, Jupyter Lab 4</i> as the platform identifier on an Amazon SageMaker Notebook instance.</li>\n",
    "                <li> (or)\n",
    "                <li>With <i>Data Science 3.0</i> as the image on an Amazon SageMaker Studio Notebook.</li>\n",
    "            <ul>\n",
    "        </li>\n",
    "        <li>At the time of this writing, the most relevant latest version of the Kernel for running this notebook,\n",
    "            <ul>\n",
    "                <li>On an Amazon SageMaker Notebook instance was <i>conda_python3</i></li>\n",
    "                <li>On an Amazon SageMaker Studio Notebook was <i>Python 3</i></li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e88d20",
   "metadata": {},
   "source": [
    "**Table of Contents:**\n",
    "\n",
    "1. [Complete prerequisites](#Complete%20prerequisites)\n",
    "\n",
    "    1. [Check and configure access to the Internet](#Check%20and%20configure%20access%20to%20the%20Internet)\n",
    "\n",
    "    2. [Install required software libraries](#Install%20required%20software%20libraries)\n",
    "    \n",
    "    3. [Configure logging](#Configure%20logging)\n",
    "        \n",
    "        1. [System logs (Optional)](#Configure%20system%20logs%20(Optional))\n",
    "        \n",
    "        2. [Application logs](#Configure%20application%20logs)\n",
    "    \n",
    "    4. [Organize imports](#Organize%20imports)\n",
    "    \n",
    "    5. [Set AWS Region and boto3 config](#Set%20AWS%20Region%20and%20boto3%20config)\n",
    "    \n",
    "    6. [Create common objects](#Create%20common%20objects)\n",
    "    \n",
    "    7. [Check and create the metadata table](#Check%20and%20create%20the%20metadata%20table)\n",
    "    \n",
    "    8. [Check and configure security permissions](#Check%20and%20configure%20security%20permissions)\n",
    "  \n",
    "    9. [Setup BDA](#Setup%20BDA)\n",
    "\n",
    " 2. [Generate and store the file tags in the metadata table](#Generate%20and%20store%20the%20file%20tags%20in%20the%20metadata%20table)\n",
    " \n",
    " 3. [Retrieve the file tags from the metadata table](#Retrieve%20the%20file%20tags%20from%20the%20metadata%20table)\n",
    " \n",
    " 4. [Cleanup](#Cleanup)\n",
    " \n",
    " 5. [Conclusion](#Conclusion)\n",
    " \n",
    " 6. [Frequently Asked Questions (FAQs)](#FAQs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9fb9d3",
   "metadata": {},
   "source": [
    "##  1. Complete prerequisites <a id ='Complete%20prerequisites'> </a>\n",
    "\n",
    "Check and complete the prerequisites."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e85c39b",
   "metadata": {},
   "source": [
    "###  A. Check and configure access to the Internet <a id ='Check%20and%20configure%20access%20to%20the%20Internet'> </a>\n",
    "This notebook requires outbound access to the Internet to download the required software updates and to download the dataset.  You can either provide direct Internet access (default) or provide Internet access through an [Amazon VPC](https://aws.amazon.com/vpc/).  For more information on this, refer [here](https://docs.aws.amazon.com/sagemaker/latest/dg/appendix-notebook-and-internet-access.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820efd56",
   "metadata": {},
   "source": [
    "### B. Install required software libraries <a id ='Install%20required%20software%20libraries'> </a>\n",
    "This notebook requires the following libraries:\n",
    "* [SageMaker Python SDK version 2.x](https://sagemaker.readthedocs.io/en/stable/v2.html)\n",
    "* [Python 3.10.x](https://www.python.org/downloads/release/python-3100/)\n",
    "* [Boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)\n",
    "\n",
    "Run the following cell to install the required libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb373af",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">  \n",
    "    <b>Note:</b> At the end of the installation, the Kernel will be forcefully restarted immediately. Please wait 10 seconds for the kernel to come back before running the next cell.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7256d4fc-0361-4cee-a548-d9b7e355824f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install boto3==1.35.94\n",
    "!pip install sagemaker==2.237.1\n",
    "\n",
    "import IPython\n",
    "\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b3c44f",
   "metadata": {},
   "source": [
    "### C. Configure logging <a id ='Configure%20logging'> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd5ee37",
   "metadata": {},
   "source": [
    "####  a. System logs (Optional) <a id='Configure%20system%20logs%20(Optional)'></a>\n",
    "\n",
    "System logs refers to the logs generated by the notebook's interactions with the underlying notebook instance. Some examples of these are the logs generated when loading or saving the notebook.\n",
    "\n",
    "These logs are automatically setup when the notebook instance is launched.\n",
    "\n",
    "These logs can be accessed through the [Amazon CloudWatch Logs](https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html) console in the same AWS Region where this notebook is running.\n",
    "* When running this notebook in an Amazon SageMaker Notebook instance, navigate to the following location,\n",
    "    * <i>CloudWatch > Log groups > /aws/sagemaker/NotebookInstances > {notebook-instance-name}/jupyter.log</i>\n",
    "* When running this notebook in an Amazon SageMaker Studio Notebook, navigate to the following locations,\n",
    "    * <i>CloudWatch > Log groups > /aws/sagemaker/studio > {sagmaker-domain-name}/{user-name}/KernelGateway/{notebook-instance-name}</i>\n",
    "    * <i>CloudWatch > Log groups > /aws/sagemaker/studio > {sagmaker-domain-name}/{user-name}/JupyterServer/default</i>\n",
    "\n",
    "If you want to find out the name of the underlying instance where this notebook is running, uncomment the following code cell and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c99c519",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import json\n",
    "\n",
    "notebook_name = ''\n",
    "resource_metadata_path = '/opt/ml/metadata/resource-metadata.json'\n",
    "with open(resource_metadata_path, 'r') as metadata:\n",
    "    notebook_name = (json.load(metadata))['ResourceName']\n",
    "print(\"Notebook instance name: '{}'\".format(notebook_name))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cc0025",
   "metadata": {},
   "source": [
    "####  b. Application logs <a id='Configure%20application%20logs'></a>\n",
    "\n",
    "Application logs refers to the logs generated by running the various code cells in this notebook. To set this up, instantiate the [Python logging service](https://docs.python.org/3/library/logging.html) by running the following cell. You can configure the default log level and format as required.\n",
    "\n",
    "By default, this notebook will only print the logs to the corresponding cell's output console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf96e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "# Set the logging level and format\n",
    "log_level = logging.INFO\n",
    "log_format = '%(asctime)s - %(levelname)s - %(message)s'\n",
    "logging.basicConfig(level=log_level, format=log_format)\n",
    "\n",
    "# Save these in the environment variables for use in the helper scripts\n",
    "os.environ['LOG_LEVEL'] = str(log_level)\n",
    "os.environ['LOG_FORMAT'] = log_format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3bb063",
   "metadata": {},
   "source": [
    "###  D. Organize imports <a id ='Organize%20imports'> </a>\n",
    "\n",
    "Organize all the library and module imports for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764a06b9-812c-4dad-a652-1cb34aa9d8b7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import sys\n",
    "from botocore.config import Config\n",
    "\n",
    "# Import the helper functions from the 'scripts' folder\n",
    "sys.path.append(os.path.join(os.getcwd(), \"scripts\"))\n",
    "#logging.info(\"Updated sys.path: {}\".format(sys.path))\n",
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48ba26b",
   "metadata": {},
   "source": [
    "Print the installed versions of some of the important libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb23f2d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logging.info(\"Python version : {}\".format(sys.version))\n",
    "logging.info(\"Boto3 version : {}\".format(boto3.__version__))\n",
    "logging.info(\"SageMaker Python SDK version : {}\".format(sagemaker.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f63309",
   "metadata": {},
   "source": [
    "###  E. Set AWS Region and boto3 config <a id ='Set%20AWS%20Region%20and%20boto3%20config'> </a>\n",
    "\n",
    "Get the current AWS Region (where this notebook is running) and the SageMaker Session. These will be used to initialize some of the clients to AWS services using the boto3 APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a746eb15",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">  \n",
    "<b>Note:</b> All the AWS services used by this notebook except Amazon Bedrock Data Automation and Amazon S3 will use the current AWS Region. For those two services, follow the guidance in the next cell.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de7be30",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> At the time of writing this notebook, Amazon Bedrock Data Automation was only available in the <mark>us-west-2 (Oregon)</mark> AWS Region. If you are running this notebook from any other AWS Region, then you have to change the Amazon Bedrock Data Automation (BDA) client's region and/or endpoint URL parameters to that AWS Region. As BDA expects the Amazon S3 buckets to be in the same AWS Region, the S3 client's and/or endpoint URL parameters will be the same as the BDA client.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a6cb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the AWS Region, SageMaker Session and IAM Role references\n",
    "my_session = boto3.session.Session()\n",
    "logging.info(\"SageMaker Session: {}\".format(my_session))\n",
    "my_iam_role = sagemaker.get_execution_role()\n",
    "logging.info(\"Notebook IAM Role: {}\".format(my_iam_role))\n",
    "my_region = my_session.region_name\n",
    "logging.info(\"Current AWS Region: {}\".format(my_region))\n",
    "\n",
    "# Explicity set the AWS Region for BDA and S3 clients\n",
    "bda_region = \"us-west-2\"\n",
    "s3_region = \"us-west-2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8484fc",
   "metadata": {},
   "source": [
    "Set the timeout and retry configurations that will be applied to all the boto3 clients used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037155d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the standard time out limits in the boto3 client from 1 minute to 3 minutes\n",
    "# and set the retry limits\n",
    "my_boto3_config = Config(\n",
    "    connect_timeout = (60 * 3),\n",
    "    read_timeout = (60 * 3),\n",
    "    retries = {\n",
    "        'max_attempts': 10,\n",
    "        'mode': 'standard'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12579ad7",
   "metadata": {},
   "source": [
    "###  F. Create common objects <a id='Create%20common%20objects'></a>\n",
    "\n",
    "Specify the input and output S3 bucket names, and initialize other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693def8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the input Amazon S3 bucket name that will contain all the source files\n",
    "# and the output S3 bucket name that will contain the files generated by BDA\n",
    "s3_input_bucket_name = \"<The name of the input S3 bucket>\"\n",
    "s3_output_bucket_name = \"<The name of the output S3 bucket>\"\n",
    "\n",
    "# Specify the name of the DynamoDB table that will contain the metadata\n",
    "ddb_table_name = \"<The name of the DynamoDB table that will store the metadata>\"\n",
    "\n",
    "# Specify the names of the BDA components\n",
    "image_blueprint_name = \"FileTaggerImageBlueprint\"\n",
    "image_blueprint_desc = \"This blueprint is to extract key information from general images to support image search.\"\n",
    "image_doc_class = \"general images\"\n",
    "image_blueprint_version_arn = \"\"\n",
    "document_blueprint_name = \"FileTaggerDocumentBlueprint\"\n",
    "document_blueprint_desc = \"This document represents a letter sent from a person or company to another.\"\n",
    "document_doc_class = \"Letters\"\n",
    "document_blueprint_version_arn = \"\"\n",
    "bda_project_name = \"FileTaggerProject\"\n",
    "\n",
    "# Specify and create the required output directories\n",
    "data_dir = os.path.join(os.getcwd(), \"data\")\n",
    "os.makedirs(data_dir, exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f230ff",
   "metadata": {},
   "source": [
    "###  G. Check and create the metadata table <a id ='Check%20and%20create%20the%20metadata%20table'> </a>\n",
    "\n",
    "Run the following cell to check and create the specified Amazon DynamoDB table in the current AWS Region. This table will contain the metadata for the objects stored in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a586c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Amazon DynamoDB clients\n",
    "ddb_client = boto3.client(\"dynamodb\", region_name = my_region, config = my_boto3_config)\n",
    "ddb_resource = boto3.resource(\"dynamodb\", region_name = my_region, config = my_boto3_config)\n",
    "\n",
    "# Note: 'check_and_create_metadata_table' is available through ./scripts/helper_functions.py\n",
    "check_and_create_metadata_table(ddb_client, ddb_table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2ee077",
   "metadata": {},
   "source": [
    "###  H. Check and configure security permissions <a id ='Check%20and%20configure%20security%20permissions'> </a>\n",
    "This notebook uses the IAM role attached to the underlying notebook instance.  To view the name of this role, run the following cell. This IAM role should have the following permissions,\n",
    "1. Full access to Amazon Bedrock Data Automation (BDA).\n",
    "2. Full access for BDA to read from the Amazon S3 bucket that contains the files to be tagged.\n",
    "3. Full access for BDA to the Amazon S3 bucket that will contain the files generated by BDA.\n",
    "4. Full access to this notebook to read and write to the input and output Amazon S3 buckets.\n",
    "5. Full access to read and write to the Amazon DynamoDB table that will contain the metadata.\n",
    "6. Access to write to Amazon CloudWatch Logs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9688f610",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b>  If you are running this notebook as part of a workshop session, by default, all these permissions will be setup.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16baab7",
   "metadata": {},
   "source": [
    "Run the following cell to print the details of the IAM role attached to the underlying notebook instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c64186",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the IAM role ARN and console URL\n",
    "logging.info(\"This notebook's IAM role is '{}'\".format(my_iam_role))\n",
    "arn_parts = my_iam_role.split('/')\n",
    "logging.info(\"Details of this IAM role are available at https://{}.console.aws.amazon.com/iamv2/home?region={}#/roles/details/{}?section=permissions\"\n",
    "             .format(my_region, my_region, arn_parts[len(arn_parts) - 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1357a9-eb49-4fd1-a4ef-23e515662bcd",
   "metadata": {},
   "source": [
    "### I. Setup BDA <a id ='Setup%20BDA'> </a>\n",
    "\n",
    "Run the following cells to setup Amazon Bedrock Data Automation (BDA) components - blueprints, blueprint versions, and projects. The configuration for inferring content from document and image files will be defined in these components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7606363-18b7-42a0-9077-4ac78fae160a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> At the time of writing this notebook, BDA supported only <b>PDF, JPEG, PNG, TIFF</b> file extensions for Document/Image Modality. So this notebook will ignore the processing of any other file present in the input S3 bucket.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ad7509-4dd8-49be-bf9d-335349a74c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the BDA clients\n",
    "bda_client = boto3.client(\"bedrock-data-automation\", region_name = bda_region, config = my_boto3_config)\n",
    "bda_rt_client = boto3.client(\"bedrock-data-automation-runtime\", region_name = bda_region, config = my_boto3_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627f1fc7-927e-42b4-b410-97740efbbd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: 'check_and_create_bda_blueprint', 'check_and_create_bda_blueprint_version' and 'check_and_create_bda_project'\n",
    "# are available through ./scripts/helper_functions.py\n",
    "image_blueprint_arn = check_and_create_bda_blueprint(bda_client, image_blueprint_name, 'IMAGE', image_blueprint_desc, image_doc_class)\n",
    "image_blueprint_version_arn = check_and_create_bda_blueprint_version(bda_client, image_blueprint_arn, image_blueprint_version_arn)\n",
    "document_blueprint_arn = check_and_create_bda_blueprint(bda_client, document_blueprint_name, 'DOCUMENT', document_blueprint_desc, document_doc_class)\n",
    "document_blueprint_version_arn = check_and_create_bda_blueprint_version(bda_client, document_blueprint_arn, document_blueprint_version_arn)\n",
    "bda_project_arn = check_and_create_bda_project(bda_client, bda_project_name,\n",
    "                                               document_blueprint_arn, document_blueprint_version_arn,\n",
    "                                               image_blueprint_arn, image_blueprint_version_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626b4340-f407-4521-8ea6-b0c5271c0867",
   "metadata": {},
   "source": [
    "## 2. Generate and store the file tags in the metadata table <a id ='Generate%20and%20store%20the%20file%20tags%20in%20the%20metadata%20table'> </a>\n",
    "\n",
    "Running the following cell performs the following:\n",
    "\n",
    "1. Gets the list of all the files in the input S3 bucket.\n",
    "2. For each file, the file extension is checked to be one of these -> 'pdf', 'jpeg', 'png', 'tiff'. If not, that file will be ignored. If yes, then the following steps will be executed:\n",
    "   1. Amazon Bedrock Data Automation (BDA) will be invoked which will,\n",
    "       1. Read the specified file from the input S3 bucket.\n",
    "       2. Processe the file as per the configuration in the BDA project.\n",
    "       3. If there are any matching blueprints found in the project, they will also be applied; else the blueprints will be ignored.\n",
    "       4. Store the output results (metadata file, standard output, custom output) in the output S3 bucket.\n",
    "    2. From the BDA results, the required data fields will be extracted to create the file tags.\n",
    "    3. The file tags will be stored as a JSON type metadata in the specified metadata (DynamoDB) table using the file's S3 URI as the key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa53c415-8e49-41d7-8101-27034ed77864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: 'list_s3_files' is available through ./scripts/helper_functions.py\n",
    "# Get the list of files that exist in the input S3 bucket\n",
    "input_files = list_s3_files(s3_input_bucket_name, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe436eb-5cce-4306-a92a-d87990dd7374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the list to capture the processed files\n",
    "processed_s3_uris = []\n",
    "\n",
    "# Loop through the input files and process\n",
    "for input_file in input_files:\n",
    "    input_s3_file = '{}/{}'.format(s3_input_bucket_name, input_file)\n",
    "    # Note: 'get_file_name_and_extension' is available through ./scripts/helper_functions.py\n",
    "    input_file_name, input_file_extension = get_file_name_and_extension(input_file)\n",
    "    # Filter out unsupported files\n",
    "    if input_file_extension.lower() in ['pdf', 'jpeg', 'png', 'tiff']:\n",
    "        # Invoke BDA on the input file\n",
    "        logging.info(\"Processing file '{}'...\".format(input_s3_file))\n",
    "        # Note: 'process_bda_invocation' is available through ./scripts/helper_functions.py\n",
    "        bda_invocation_id = process_bda_invocation(bda_rt_client,\n",
    "                                                   input_s3_file,\n",
    "                                                   s3_output_bucket_name,\n",
    "                                                   bda_project_arn)\n",
    "        if len(bda_invocation_id) == 0:\n",
    "            logging.error(\"Could not complete processing file. Invocation id not available.\")\n",
    "        else:\n",
    "            # Note: 'process_bda_invocation' is available through ./scripts/helper_functions.py\n",
    "            file_metadata = process_bda_result(bda_invocation_id, s3_output_bucket_name, data_dir)\n",
    "            # Insert/update the metadata table\n",
    "            file_s3_uri = 's3://{}/{}'.format(s3_input_bucket_name, input_file)\n",
    "            # Note: 'write_to_metadata_table' is available through ./scripts/helper_functions.py\n",
    "            write_to_metadata_table(ddb_resource, ddb_table_name, file_s3_uri, file_metadata)\n",
    "            # Add to the processed file list\n",
    "            processed_s3_uris.append(file_s3_uri)\n",
    "            logging.info(\"Completed processing file.\")\n",
    "    else:\n",
    "        logging.warning(\"Ignored processing of unsupported file '{}'.\".format(input_s3_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0ef817",
   "metadata": {},
   "source": [
    "## 3. Retrieve the file tags from the metadata table <a id='Retrieve%20the%20file%20tags%20from%20the%20metadata%20table'></a>\n",
    "\n",
    "Loop through all the processed files, retrieve their corresponding file tags from the metadata table and print them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7400f5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the processed files\n",
    "for processed_s3_uri in processed_s3_uris:\n",
    "    # Retrieve the metadata\n",
    "    # Note: 'retrieve_from_metadata_table' is available through ./scripts/helper_functions.py\n",
    "    logging.info(\"File: {}, Metadata = {}\".format(processed_s3_uri, retrieve_from_metadata_table(ddb_resource,\n",
    "                                                                                                 ddb_table_name,\n",
    "                                                                                                 processed_s3_uri)))   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a1216a",
   "metadata": {},
   "source": [
    "## 4. Cleanup <a id='Cleanup'></a>\n",
    "\n",
    "As a best practice, you should delete AWS resources that are no longer required.  This will help you avoid incurring unncessary costs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c018f3f7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> If you are running this notebook as part of a workshop session, by default, all resources will be cleaned up at the end of the session. If you are running this notebook outside of a workshop session, you can cleanup the resources associated with this notebook by uncommenting the following code cell and running it.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb865606",
   "metadata": {},
   "source": [
    "Running the following cell will delete the following resources:\n",
    "* The BDA blueprints and projects that were created.\n",
    "* The DynamoDB table that was created to store the metadata.\n",
    "* The files that were uploaded to the S3 bucket; not the S3 bucket itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6923a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Note: 'delete_bda_blueprint' and 'delete_bda_project' available through ./scripts/helper_functions.py\n",
    "delete_bda_project(bda_client, bda_project_arn)\n",
    "delete_bda_blueprint(bda_client, image_blueprint_arn, image_blueprint_version_arn)\n",
    "delete_bda_blueprint(bda_client, image_blueprint_arn, '')\n",
    "delete_bda_blueprint(bda_client, document_blueprint_arn, document_blueprint_version_arn)\n",
    "delete_bda_blueprint(bda_client, document_blueprint_arn, '')\n",
    "\n",
    "# Note: 'delete_metadata_table' is available through ./scripts/helper_functions.py\n",
    "delete_metadata_table(ddb_client, ddb_table_name)\n",
    "\n",
    "# Create the Amazon S3 client\n",
    "s3_client = boto3.client(\"s3\", region_name = my_region, config = my_boto3_config)\n",
    "# Loop through the downloaded files\n",
    "for input_file in input_files:\n",
    "    # Note: 'delete_from_s3' is available through ./scripts/helper_functions.py\n",
    "    delete_from_s3(s3_client, s3_input_bucket_name, input_file)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bd52a5",
   "metadata": {},
   "source": [
    "## 5. Conclusion <a id='Conclusion'></a>\n",
    "\n",
    "We have now seen how to build a content-based file tagging system using Amazon Bedrock Data Automation (BDA) for document and image content. In the process, we also saw how easy it is to use BDA for custom data extraction from multi-modal content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf266cf",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## 6. Frequently Asked Questions (FAQs) <a id='FAQs'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf2c50f",
   "metadata": {},
   "source": [
    "**Q: What AWS services are used in this notebook?**\n",
    "\n",
    "Amazon Bedrock, Amazon DynamoDB, Amazon S3, AWS Identity and Access Management (IAM), Amazon CloudWatch, and Amazon SageMaker Notebook instance (or) Amazon SageMaker Studio Notebook depending on what you use to run the notebook.\n",
    "\n",
    "**Q: Where can I access the FAQs for Amazon Bedrock?**\n",
    "\n",
    "Go [here](https://aws.amazon.com/bedrock/faqs/).\n",
    "\n",
    "**Q: What models are supported by Amazon Bedrock?**\n",
    "\n",
    "Go [here](https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html).\n",
    "\n",
    "**Q: Where can I find customer references for Amazon Bedrock?**\n",
    "\n",
    "Go [here](https://aws.amazon.com/bedrock/testimonials/).\n",
    "\n",
    "**Q: Where can I find pricing information for the AWS services used in this notebook?**\n",
    "\n",
    "- Amazon Bedrock pricing - go [here](https://aws.amazon.com/bedrock/pricing/).\n",
    "- Amazon DynamoDB pricing - go [here](https://aws.amazon.com/dynamodb/pricing/).\n",
    "- Amazon S3 pricing - go [here](https://aws.amazon.com/s3/pricing/).\n",
    "- AWS Identity and Access Management (IAM) pricing - free.\n",
    "- Amazon CloudWatch pricing - go [here](https://aws.amazon.com/cloudwatch/pricing/).\n",
    "- Amazon SageMaker Notebook instance (or) Amazon SageMaker Studio Notebook pricing - go [here](https://aws.amazon.com/sagemaker/pricing/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3c1e7f-dabf-4e1f-97e9-15333f538540",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
